{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interlingua\n",
    "\n",
    "Hossein Askari-2024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking that all requirements are installed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import transformers\n",
    "from pyvis.network import Network\n",
    "\n",
    "# Print installed versions to verify\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"Pandas version:\", pd.__version__)\n",
    "print(\"Matplotlib version:\", plt.__version__)\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Transformers version:\", transformers.__version__)\n",
    "\n",
    "\n",
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "print(\"CUDA Available:\", cuda_available)\n",
    "if cuda_available:\n",
    "    print(\"CUDA Version:\", torch.version.cuda)\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "    print(\"CUDA Device Name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available. Please check your installation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking GPTs functionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Choose a GPT-Neo model. (can also try \"EleutherAI/gpt-neo-2.7B\" for larger model)\n",
    "model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
    "\n",
    "# Initialize the pipeline with the chosen model\n",
    "generator = pipeline('text-generation', model=model_name)\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"This is a heartfelt love letter. My dear love, \"\n",
    "\n",
    "# Generate text\n",
    "outputs = generator(prompt, max_length=100, temperature=0.8, do_sample=True, top_p=0.95)\n",
    "\n",
    "# Print the generated text\n",
    "generated_text = outputs[0][\"generated_text\"]\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"a love letter to my beloved sara\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "# Generate text\n",
    "output = model.generate(input_ids, max_length=300, num_return_sequences=1, no_repeat_ngram_size=2, temperature=0.7)\n",
    "\n",
    "# Decode and Print the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Evaluation model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "import os\n",
    "\n",
    "# Initialize BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to read texts from a folder and return as a list\n",
    "def read_texts_from_folder(folder_path):\n",
    "    texts = []\n",
    "    for filename in sorted(os.listdir(folder_path), key=lambda x: os.path.getmtime(os.path.join(folder_path, x))):\n",
    "        filepath = os.path.join(folder_path, filename)\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            texts.append(file.read())\n",
    "    return texts\n",
    "\n",
    "# Convert text to embedding\n",
    "def text_to_embedding(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "    return embeddings.squeeze().numpy()\n",
    "\n",
    "# Calculate cosine similarity between two embeddings\n",
    "def calculate_similarity(embedding1, embedding2):\n",
    "    embedding1 = np.asarray(embedding1).squeeze()\n",
    "    embedding2 = np.asarray(embedding2).squeeze()\n",
    "    return 1 - cosine(embedding1, embedding2)\n",
    "\n",
    "# Load reference texts from files \n",
    "reference_folder = './data/external/LetterRefTrue'  \n",
    "reference_texts = read_texts_from_folder(reference_folder)\n",
    "reference_embeddings = [text_to_embedding(text, tokenizer, model) for text in reference_texts]\n",
    "\n",
    "# Function to evaluate a single text input against reference embeddings\n",
    "def evaluate_text(text, reference_embeddings, tokenizer, model, threshold=0.78):\n",
    "    new_embedding = text_to_embedding(text, tokenizer, model)\n",
    "    similarities = [calculate_similarity(new_embedding, ref_emb) for ref_emb in reference_embeddings]\n",
    "    avg_similarity = np.mean(similarities)\n",
    "    classification = 'True' if avg_similarity >= threshold else 'False'\n",
    "    return classification, avg_similarity\n",
    "\n",
    "# Example usage with a generated text\n",
    "generated_text = \"generated text here.\"  # This is an example; replace with generated text\n",
    "classification, avg_similarity = evaluate_text(generated_text, reference_embeddings, tokenizer, model)\n",
    "print(f\"Generated Text: Average Similarity = {avg_similarity:.4f} -> Classified as {classification}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline, BertTokenizer, BertModel\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def read_texts_from_folder(folder_path):\n",
    "    texts = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        filepath = os.path.join(folder_path, filename)\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            texts.append(file.read())\n",
    "    return texts\n",
    "\n",
    "# BERT-based text evaluation setup\n",
    "def text_to_embedding(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "    return embeddings.squeeze().numpy()\n",
    "\n",
    "def calculate_similarity(embedding1, embedding2):\n",
    "    embedding1 = np.asarray(embedding1).squeeze()\n",
    "    embedding2 = np.asarray(embedding2).squeeze()\n",
    "    return 1 - cosine(embedding1, embedding2)\n",
    "\n",
    "# Custom Environment for text generation\n",
    "class TextGenerationEnv(gym.Env):\n",
    "    \"\"\"Custom Environment for text generation that follows gym interface\"\"\"\n",
    "    def __init__(self, reference_embeddings, model_type='gpt2', max_length=300, num_prompts=2):\n",
    "        super(TextGenerationEnv, self).__init__()\n",
    "        self.model_type = model_type\n",
    "        # Initialize tokenizer and model appropriately for GPT-2 and GPT-Neo\n",
    "        if model_type == 'gpt2':\n",
    "            self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "            self.model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        else:  # GPT-Neo\n",
    "            self.tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "            self.model = pipeline('text-generation', model=\"EleutherAI/gpt-neo-1.3B\")\n",
    "        \n",
    "        self.reference_embeddings = reference_embeddings\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.action_space = spaces.Discrete(num_prompts)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(768,), dtype=np.float32)\n",
    "        \n",
    "        self.prompts = [\"A love letter to my beloved\", \"A letter to my love\"]\n",
    "    def seed(self, seed=None):\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    def step(self, action):\n",
    "        prompt = self.prompts[action]\n",
    "        generated_text = self.generate_text(prompt)\n",
    "        reward = self.evaluate_text(generated_text)\n",
    "        observation = text_to_embedding(generated_text, self.bert_tokenizer, self.bert_model)\n",
    "        done = True\n",
    "\n",
    "        return observation, reward, done, {}\n",
    "\n",
    "    def evaluate_text(self, text):\n",
    "        new_embedding = text_to_embedding(text, self.bert_tokenizer, self.bert_model)\n",
    "        similarities = [calculate_similarity(new_embedding, ref_emb) for ref_emb in self.reference_embeddings]\n",
    "        avg_similarity = np.mean(similarities)\n",
    "        return avg_similarity\n",
    "\n",
    "    def generate_text(self, prompt):\n",
    "        if self.model_type == 'gpt2':\n",
    "            input_ids = self.tokenizer.encode(prompt, return_tensors='pt')\n",
    "            output = self.model.generate(input_ids, max_length=self.max_length, num_return_sequences=1)\n",
    "            return self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        else:  # GPT-Neo\n",
    "            response = self.model(prompt, max_length=self.max_length)\n",
    "            return response[0][\"generated_text\"]\n",
    "\n",
    "    def reset(self):\n",
    "        initial_text = \"Resetting environment\"\n",
    "        observation = text_to_embedding(initial_text, self.bert_tokenizer, self.bert_model)\n",
    "        return observation\n",
    "\n",
    "    def render(self, mode='console'):\n",
    "        if mode != 'console':\n",
    "            raise NotImplementedError()\n",
    "        print(\"Rendering...\")\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def save_generated_texts(self, filepath):\n",
    "        \"\"\"Saves the generated texts to a file.\"\"\"\n",
    "        with open(filepath, 'w', encoding='utf-8') as file:\n",
    "            for text in self.generated_texts:\n",
    "                file.write(text + \"\\n\\n\")\n",
    "\n",
    "# Training setup\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "reference_texts = read_texts_from_folder('./data/external/LetterRefTrue')\n",
    "reference_embeddings = [text_to_embedding(text, bert_tokenizer, bert_model) for text in reference_texts]\n",
    "\n",
    "env = TextGenerationEnv(reference_embeddings=reference_embeddings, model_type='gpt2')\n",
    "env = make_vec_env(lambda: env, n_envs=1)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, n_steps=10)\n",
    "model.learn(total_timesteps=100, progress_bar=True)\n",
    "single_env = env.envs[0].env\n",
    "\n",
    "# to save the outpot texts\n",
    "single_env.save_generated_texts('generated_texts.txt')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
